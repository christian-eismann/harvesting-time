{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import haversine as hs\n",
    "import requests\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO, StringIO\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read crop data from offical data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read crop yield data\n",
    "ert_raw = pd.read_csv('secondary_data/41241-01-03-4_flat.csv', sep=';', encoding='iso8859_15')\n",
    "\n",
    "# rename labels\n",
    "ert = ert_raw[['Zeit', '1_Auspraegung_Code', '2_Auspraegung_Label', 'ERT001__Hektarertraege__dt/ha']].copy()\n",
    "ert.columns = ['year', 'district', 'crop', 'yield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all district codes with less than 5 digits\n",
    "# all other represent higher administrative regions\n",
    "\n",
    "drop = np.unique([x for x in ert['district'] if (len(x)<5)])\n",
    "drop_index = []\n",
    "for d in drop:\n",
    "    drop_index.append(ert[ert['district'] == d].index)\n",
    "drop_index = np.array(drop_index)\n",
    "drop_index = drop_index.flatten()\n",
    "\n",
    "ert.drop(index=drop_index, inplace=True)\n",
    "\n",
    "# replace missing values with nan\n",
    "# replace comma with dots\n",
    "ert['yield'].replace({'.': np.nan, '-': np.nan, 'x': np.nan, '/': np.nan, '...': np.nan}, inplace=True)\n",
    "ert['yield'] = ert['yield'].str.replace(',', '.')\n",
    "ert['yield'] = pd.to_numeric(ert['yield'])\n",
    "\n",
    "# cropping Region_Code to 5 digits\n",
    "ert['district'] = [x[:5] for x in ert['district']]\n",
    "\n",
    "# creating some lists for later use\n",
    "crops = ert['crop'].unique().tolist()\n",
    "gmky5 = ert['district'].unique().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing in dictionary with structure data['district code]['crops']\n",
    "data = {}\n",
    "for district in gmky5:\n",
    "    x = ert[ert['district']==district]\n",
    "    if len(x)==220:    \n",
    "        x = x.pivot(index='year', columns='crop', values='yield')\n",
    "    else:\n",
    "        x = x.groupby(['crop', 'year']).agg('mean').unstack('crop')['yield']\n",
    "    crop = {}\n",
    "    crop['crops'] = x\n",
    "    data[district] = crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get geographial middle point of districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'secondary_data/'\n",
    "files = [\n",
    "    str(path+'31122007_Auszug_GV.xlsx'), str(path+'31122008_Auszug_GV.xlsx'), str(path+'31122009_Auszug_GV.xlsx'),\n",
    "    str(path+'31122010_Auszug_GV.xlsx'), str(path+'31122011_Auszug_GV.xlsx'), str(path+'31122012_Auszug_GV.xlsx'),\n",
    "    str(path+'31122013_Auszug_GV.xlsx'), str(path+'31122014_Auszug_GV.xlsx'), str(path+'31122015_Auszug_GV.xlsx'),\n",
    "    str(path+'31122016_Auszug_GV.xlsx'), str(path+'31122017_Auszug_GV.xlsx'), str(path+'31122018_Auszug_GV.xlsx'),\n",
    "    str(path+'31122019_Auszug_GV.xlsx'), str(path+'31122020_Auszug_GV.xlsx')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading excel files with coordinates of all municipalties,\n",
    "# aggregating on district level by computing means of lat lon positions\n",
    "dstrcts_ll = {}\n",
    "\n",
    "for file, year in zip(files, np.arange(2006, 2021)):\n",
    "        \n",
    "    # load data\n",
    "    df = pd.read_excel(file, sheet_name=1)\n",
    "\n",
    "    df = df.iloc[:, [0, 2, 3, 4, 14, 15]]\n",
    "    df.columns = ['Satz', 'k1', 'k2', 'k3', 'lon', 'lat']\n",
    "\n",
    "    # cut upper and lower end\n",
    "    cut_to = df.index[np.where(df['Satz']=='10')[0][0]]\n",
    "    df.drop(np.arange(0, (cut_to+1)), inplace=True)\n",
    "\n",
    "    try:\n",
    "        cut_from = df.index[df['Satz'].isnull()][0]\n",
    "        df.drop(np.arange(cut_from, (df.index[-1]+1)), inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # build GemKey5 through concat strings\n",
    "    GemKey5 = [str(df.iloc[i, 1]) + str(df.iloc[i, 2]) + str(df.iloc[i, 3]) for i in np.arange(0, len(df))]\n",
    "    df['GemKey5'] = GemKey5\n",
    "\n",
    "    # mean values of lat and lon through districts\n",
    "    try: \n",
    "        df['lat'] = pd.to_numeric(df['lat'].str.replace(',', '.'))\n",
    "        df['lon'] = pd.to_numeric(df['lon'].str.replace(',', '.'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    dx = df[['lat', 'lon']].groupby(df['GemKey5']).agg('mean').dropna()\n",
    "\n",
    "    dstrcts_ll[year] = dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with districts (keys) and lat lon\n",
    "position = pd.DataFrame(columns=['lat', 'lon'])\n",
    "for year in dstrcts_ll.keys():\n",
    "    position = position.append(dstrcts_ll[year])\n",
    "position = position.groupby(position.index).agg('mean')\n",
    "dstrcts_ll = {district:[position.loc[district, 'lat'], position.loc[district, 'lon']] for district in position.index}\n",
    "\n",
    "# add position to dataset\n",
    "for gmky in data.keys():\n",
    "    try:\n",
    "        data[gmky]['position'] = dstrcts_ll[gmky]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find near weather stations of districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read coordinates of weather stations from file\n",
    "# source: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/annual/kl/historical/KL_Jahreswerte_Beschreibung_Stationen.txt\n",
    "\n",
    "w_stations = pd.read_fwf('secondary_data/KL_Monatswerte_Beschreibung_Stationen.txt', \n",
    "                         colspecs=[(0, 5), (6, 14), (15, 23), (24, 38), (43, 49), (53, 60), (61, 100), (102, 123)], \n",
    "                         encoding='iso8859_15')\n",
    "\n",
    "w_stations.columns=['Stations_id', 'von_datum', 'bis_datum', 'Stationshoehe', \n",
    "                    'geoBreite', 'geoLaenge', 'Stationsname', 'Bundesland']\n",
    "w_stations.drop(0, axis=0, inplace=True)\n",
    "\n",
    "w_stations['von_datum'] = pd.to_numeric(w_stations['von_datum'])\n",
    "w_stations['bis_datum'] = pd.to_numeric(w_stations['bis_datum'])\n",
    "w_stations['Stationshoehe'] = pd.to_numeric(w_stations['Stationshoehe'])\n",
    "w_stations['geoBreite'] = pd.to_numeric(w_stations['geoBreite'])\n",
    "w_stations['geoLaenge'] = pd.to_numeric(w_stations['geoLaenge'])\n",
    "\n",
    "# keep only those weather stations providing data from 1999 onwards\n",
    "w_stations = w_stations[w_stations['bis_datum']>19990000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_close_weather_stations(latlon, radius):\n",
    "    # center of district\n",
    "    center_district = np.array(latlon)\n",
    "    \n",
    "    # looping through list of weather stations \n",
    "    distance = np.array([hs.haversine(center_district, np.array(w_stations.iloc[i, 4:6])) for i in range(len(w_stations))])\n",
    "    distance = pd.Series(distance, index=w_stations['Stations_id'], name='distance')\n",
    "    keep = distance<=radius\n",
    "    distance = distance[keep]\n",
    "        \n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donerict: 16077\n"
     ]
    }
   ],
   "source": [
    "for district in data:\n",
    "    print(f'district: {district}', end='\\r')\n",
    "    try:\n",
    "        close_stations = find_close_weather_stations(data[str(district)]['position'], 30)\n",
    "    except:\n",
    "        close_stations = []\n",
    "    data[str(district)]['close_stations'] = close_stations\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve data from weather stations for each district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all stations needed\n",
    "relevant_stations = [data[x]['close_stations'].index for x in data.keys()]\n",
    "\n",
    "def list_index(x):\n",
    "    try:\n",
    "        y = x.values.tolist()\n",
    "    except:\n",
    "        y = []\n",
    "    return y\n",
    "\n",
    "relevant_stations = [list_index(x) for x in relevant_stations]\n",
    "relevant_stations = [item for sublist in relevant_stations for item in sublist]\n",
    "relevant_stations = np.unique(relevant_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to DWD open data portal\n",
    "path = 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/monthly/kl/historical/'\n",
    "\n",
    "# read all files in directory and put them in a list\n",
    "# weather data is provided with zip files\n",
    "site_content = requests.get(path).text\n",
    "soup = BeautifulSoup(site_content, 'html.parser').find_all('a')\n",
    "files = [x.attrs['href'] for x in soup]\n",
    "\n",
    "# Liste bereinigen: nur zip-Dateien\n",
    "zipfiles = []\n",
    "for i in files:\n",
    "    if i[-3:] == 'zip':\n",
    "        zipfiles.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# empty data frame for merging masked data\n",
    "empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function opens the zip file of a given weather station (Station ID),\n",
    "# extracts all relevant weather indicators, and returns them as dictionary\n",
    "\n",
    "def get_indicators(statID):\n",
    "    \n",
    "    statID = '_' + statID + '_'        \n",
    "    empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "    for file in zipfiles:\n",
    "        if (statID in file) == True:\n",
    "            filepath = path+file\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        filepath\n",
    "\n",
    "        goal = urlopen(filepath)\n",
    "        zippedfiles = zipfile.ZipFile(BytesIO(goal.read()))\n",
    "\n",
    "        for i in zippedfiles.namelist():\n",
    "            if ('produkt_klima_monat' in i) == True:\n",
    "                datafile = i\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # unpack files and write in data frame\n",
    "        data = zippedfiles.read(datafile).decode()\n",
    "        df = pd.read_csv(StringIO(data), sep=';')\n",
    "        df = df[df['MESS_DATUM_BEGINN']>19990000][['MESS_DATUM_BEGINN', 'MO_TT', 'MO_SD_S', 'MO_FK', 'MO_RR']]\n",
    "        df = df[df['MESS_DATUM_BEGINN']<20210000]\n",
    "\n",
    "        # get year and month\n",
    "        df['year'] = [str(x)[0:4] for x in df['MESS_DATUM_BEGINN']]\n",
    "        df['month'] = [str(x)[4:6] for x in df['MESS_DATUM_BEGINN']]\n",
    "        df['year'] = pd.to_numeric(df['year'])\n",
    "        df['month'] = pd.to_numeric(df['month'])\n",
    "\n",
    "        df.drop(['MESS_DATUM_BEGINN'], axis=1, inplace=True)\n",
    "\n",
    "        # missing values\n",
    "        df.replace({-999: np.nan}, inplace=True)\n",
    "\n",
    "        # bringing stuff together\n",
    "        df_TT = df.pivot(index='year', columns='month', values='MO_TT')\n",
    "        df_SD = df.pivot(index='year', columns='month', values='MO_SD_S')\n",
    "        df_FK = df.pivot(index='year', columns='month', values='MO_FK')\n",
    "        df_RR = df.pivot(index='year', columns='month', values='MO_RR')\n",
    "        df_TT = empty_df.fillna(df_TT)\n",
    "        df_SD = empty_df.fillna(df_SD)\n",
    "        df_FK = empty_df.fillna(df_FK)\n",
    "        df_RR = empty_df.fillna(df_RR)\n",
    "\n",
    "        # as Dictionary\n",
    "        station_indicators = {'TT': df_TT, 'SD': df_SD, 'FK': df_FK, 'RR': df_RR}\n",
    "    \n",
    "    except:\n",
    "        station_indicators = {'TT': empty_df, 'SD': empty_df, 'FK': empty_df, 'RR': empty_df}\n",
    "\n",
    "    \n",
    "    return station_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is more than one weather station in a particular district,\n",
    "# this functions comines the values\n",
    "\n",
    "def combine_stations(stat_ids):\n",
    "    \n",
    "    station_ids = stat_ids\n",
    "    station_values = {}\n",
    "\n",
    "    #if more than one weather station in district\n",
    "    if len(station_ids) > 1:\n",
    "\n",
    "        for i in station_ids:\n",
    "            station_values[i] = get_indicators(i)\n",
    "\n",
    "        TT_list = []\n",
    "        SD_list = []\n",
    "        FK_list = []\n",
    "        RR_list = []\n",
    "\n",
    "        for i in station_values.keys():\n",
    "            TT_list = TT_list + [station_values[i]['TT']]\n",
    "            SD_list = SD_list + [station_values[i]['SD']]\n",
    "            FK_list = FK_list + [station_values[i]['FK']]\n",
    "            RR_list = RR_list + [station_values[i]['RR']]\n",
    "\n",
    "        raw_df_TT = pd.concat(TT_list)\n",
    "        raw_df_SD = pd.concat(SD_list)\n",
    "        raw_df_FK = pd.concat(FK_list)\n",
    "        raw_df_RR = pd.concat(RR_list)\n",
    "\n",
    "        empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "        df_TT_full = empty_df.copy()\n",
    "        df_SD_full = empty_df.copy()\n",
    "        df_FK_full = empty_df.copy()\n",
    "        df_RR_full = empty_df.copy()\n",
    "\n",
    "        for i in empty_df.index:\n",
    "            df_TT_full.loc[i] = raw_df_TT.loc[i].mean(axis=0)\n",
    "            df_SD_full.loc[i] = raw_df_SD.loc[i].mean(axis=0)\n",
    "            df_FK_full.loc[i] = raw_df_FK.loc[i].mean(axis=0)\n",
    "            df_RR_full.loc[i] = raw_df_RR.loc[i].mean(axis=0)\n",
    "    \n",
    "    #if just one weather station in district\n",
    "    elif len(stat_ids)==0:\n",
    "        empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "        df_TT_full = empty_df\n",
    "        df_SD_full = empty_df\n",
    "        df_FK_full = empty_df\n",
    "        df_RR_full = empty_df\n",
    "    \n",
    "    else:\n",
    "        _ = get_indicators(station_ids[0])\n",
    "        df_TT_full = _['TT']\n",
    "        df_SD_full = _['SD']\n",
    "        df_FK_full = _['FK']\n",
    "        df_RR_full = _['RR']\n",
    "\n",
    "    return {'TT':df_TT_full, 'SD':df_SD_full, 'FK':df_FK_full, 'RR':df_RR_full}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>caution: the execution of the following cell will take some time!</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors on districts ['11001', '11002', '11003', '11004', '11005', '11006', '11007', '11008', '11009', '11010', '11011', '11012', '15101', '15151', '15153', '15154', '15159', '15171', '15202', '15256', '15260', '15261', '15265', '15266', '15268', '15303', '15352', '15355', '15357', '15358', '15362', '15363', '15364', '15367', '15369', '15370']\n"
     ]
    }
   ],
   "source": [
    "# retreive weather data from stations near district center\n",
    "\n",
    "errors = []\n",
    "\n",
    "for i, district in enumerate(list(data.keys())):\n",
    "    print(f'district: {district} ({i+1}/{len(data)})', end='\\r')\n",
    "    try:\n",
    "        stations = data[district]['close_stations'].index.tolist()\n",
    "        weather_data = combine_stations(stations)\n",
    "    except:\n",
    "        weather_data = []\n",
    "        errors.append(district)\n",
    "    \n",
    "    data[district]['weather_data'] = weather_data\n",
    "\n",
    "print(f'Errors on districts {errors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get Soil Moisture Index (SMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from pyproj import Transformer\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# empty data frame to fill later\n",
    "empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "# reading UFZ data set\n",
    "ds_ob = nc.Dataset('secondary_data/248980_SMI_SM_L02_Oberboden_monatlich_1951-2020_inv.nc')\n",
    "ds_gb = nc.Dataset('secondary_data/248981_SMI_SM_Lall_Gesamtboden_monatlich_1951-2020_inv.nc')\n",
    "\n",
    "# UFZ data is grid data\n",
    "# transform from lat/lon (EPSG:4326) to Gauß-Krüger Zone 4 (EPSG: 31468)\n",
    "transformer = Transformer.from_crs(4326, 31468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function get the smi value for a given latitude and longitude\n",
    "# ds = UFZ dataset\n",
    "\n",
    "def get_smi (ds, lat, lon):\n",
    "    # transform\n",
    "    nor, eas = transformer.transform(lat, lon)\n",
    "    \n",
    "    # find nearest point\n",
    "    near_north = min(ds['northing'][:], key=lambda x:abs(x-nor))\n",
    "    near_east = min(ds['easting'][:], key=lambda x:abs(x-eas))\n",
    "\n",
    "    # get index\n",
    "    ix_northing = list(ds['northing']).index(near_north)\n",
    "    ix_easting = list(ds['easting']).index(near_east)\n",
    "\n",
    "    #read SMIi for region\n",
    "    smi_raw = ds['SMI'][:, ix_northing, ix_easting].data\n",
    "\n",
    "    # sort list by years\n",
    "    smi_years = np.reshape(smi_raw, [70, 12])\n",
    "\n",
    "    # set year as index for data frame\n",
    "    smi = pd.DataFrame(smi_years, columns=[np.arange(1, 13)], index=np.arange(1951, 2021))\n",
    "\n",
    "    return smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16077\n",
      "Errors on districts: ['11001', '11002', '11003', '11004', '11005', '11006', '11007', '11008', '11009', '11010', '11011', '11012', '15101', '15151', '15153', '15154', '15159', '15171', '15202', '15256', '15260', '15261', '15265', '15266', '15268', '15303', '15352', '15355', '15357', '15358', '15362', '15363', '15364', '15367', '15369', '15370']\n"
     ]
    }
   ],
   "source": [
    "# add smi data frame to district dictionary\n",
    "\n",
    "errors = []\n",
    "for district in data.keys():\n",
    "    print(district, end='\\r')\n",
    "    try:\n",
    "        lat, lon = data[district]['position']\n",
    "        smi_ob = get_smi(ds_ob, lat, lon).loc[1999:,:]\n",
    "        smi_gb = get_smi(ds_gb, lat, lon).loc[1999:,:]\n",
    "    except:\n",
    "        smi_ob, smi_gb = [], []\n",
    "        errors.append(district)\n",
    "    \n",
    "    data[district]['SMI_OB'] = smi_ob\n",
    "    data[district]['SMI_GB'] = smi_gb\n",
    "\n",
    "print(f'\\nErrors on districts: {errors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create objects for better handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from harvestingtime import district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "district: 16077\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "dataset = {}\n",
    "\n",
    "for distr in list(data.keys()):\n",
    "    try:\n",
    "        print(f'district: {distr}', end='\\r')\n",
    "        dataset[distr] = district(data[distr])\n",
    "    except:\n",
    "        pass\n",
    "print('\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(dataset, file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7e5a1f785ed82844e2da5d30522181462e1597dbd1807cbc4c5c0cc1d5a2e0b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
