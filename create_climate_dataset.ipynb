{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data gathering and preparation\n",
    "\n",
    "<center><b>This notebook collects weather and soil data from different sources on the level of German districts.<br>The data set then can be joined with the official crop yield statstics from the official sources.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify relevant weather stations\n",
    "\n",
    "the list of weather stations in Germany can be found here:\n",
    "\n",
    "https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/annual/kl/historical/KL_Jahreswerte_Beschreibung_Stationen.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stations_id</th>\n",
       "      <th>von_datum</th>\n",
       "      <th>bis_datum</th>\n",
       "      <th>Stationshoehe</th>\n",
       "      <th>geoBreite</th>\n",
       "      <th>geoLaenge</th>\n",
       "      <th>Stationsname</th>\n",
       "      <th>Bundesland</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00003</td>\n",
       "      <td>18510101</td>\n",
       "      <td>20110331</td>\n",
       "      <td>202</td>\n",
       "      <td>50.782</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00044</td>\n",
       "      <td>19710301</td>\n",
       "      <td>20210930</td>\n",
       "      <td>44</td>\n",
       "      <td>52.933</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00052</td>\n",
       "      <td>19730101</td>\n",
       "      <td>20011231</td>\n",
       "      <td>46</td>\n",
       "      <td>53.662</td>\n",
       "      <td>10.1990</td>\n",
       "      <td>Ahrensburg-Wulfsdorf</td>\n",
       "      <td>Schleswig-Holstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00071</td>\n",
       "      <td>19861101</td>\n",
       "      <td>20191231</td>\n",
       "      <td>759</td>\n",
       "      <td>48.215</td>\n",
       "      <td>8.9784</td>\n",
       "      <td>Albstadt-Badkap</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00073</td>\n",
       "      <td>19520701</td>\n",
       "      <td>20210930</td>\n",
       "      <td>340</td>\n",
       "      <td>48.615</td>\n",
       "      <td>13.0506</td>\n",
       "      <td>Aldersbach-Kriestorf</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Stations_id  von_datum  bis_datum  Stationshoehe  geoBreite  geoLaenge  \\\n",
       "2       00003   18510101   20110331            202     50.782     6.0941   \n",
       "3       00044   19710301   20210930             44     52.933     8.2370   \n",
       "4       00052   19730101   20011231             46     53.662    10.1990   \n",
       "7       00071   19861101   20191231            759     48.215     8.9784   \n",
       "9       00073   19520701   20210930            340     48.615    13.0506   \n",
       "\n",
       "           Stationsname           Bundesland  \n",
       "2                Aachen  Nordrhein-Westfalen  \n",
       "3          Großenkneten        Niedersachsen  \n",
       "4  Ahrensburg-Wulfsdorf   Schleswig-Holstein  \n",
       "7       Albstadt-Badkap    Baden-Württemberg  \n",
       "9  Aldersbach-Kriestorf               Bayern  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_stations = pd.read_fwf('data sources/KL_Monatswerte_Beschreibung_Stationen.txt', \n",
    "                         colspecs=[(0, 5), (6, 14), (15, 23), (24, 38), (43, 49), (53, 60), (61, 100), (102, 123)], \n",
    "                         encoding='iso8859_15')\n",
    "\n",
    "w_stations.columns=['Stations_id', 'von_datum', 'bis_datum', 'Stationshoehe', \n",
    "                    'geoBreite', 'geoLaenge', 'Stationsname', 'Bundesland']\n",
    "w_stations.drop(0, axis=0, inplace=True)\n",
    "\n",
    "w_stations['von_datum'] = pd.to_numeric(w_stations['von_datum'])\n",
    "w_stations['bis_datum'] = pd.to_numeric(w_stations['bis_datum'])\n",
    "w_stations['Stationshoehe'] = pd.to_numeric(w_stations['Stationshoehe'])\n",
    "w_stations['geoBreite'] = pd.to_numeric(w_stations['geoBreite'])\n",
    "w_stations['geoLaenge'] = pd.to_numeric(w_stations['geoLaenge'])\n",
    "\n",
    "# keep only those weather stations providing data from 1999 onwards\n",
    "w_stations = w_stations[w_stations['bis_datum']>19990000]\n",
    "\n",
    "w_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of weather stations provides the geographical position and the federal state, but not the district that is needed to combine the weather data with the official crop data. So, the certain district of a weather station is retrieved from opentreetmap by using the overpass API.\n",
    "\n",
    "_The following cell needs some time to execute._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "overpass_url = 'http://overpass-api.de/api/interpreter'\n",
    "\n",
    "#requesting district code of lat/lon position of weather stations\n",
    "lk = []\n",
    "\n",
    "for station in np.arange(0, len(w_stations)):\n",
    "    \n",
    "    lat = w_stations.iloc[station, 4]\n",
    "    lon = w_stations.iloc[station, 5]\n",
    "\n",
    "    req = str('\"\"\"is_in('+str(lat)+','+str(lon) + ');area._[admin_level~\"6\"];out;\"\"\"')\n",
    "    \n",
    "    overpass_query = eval(req)\n",
    "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    try:\n",
    "        #only the first five digits for district\n",
    "        lk = lk + [soup.find(k=\"de:amtlicher_gemeindeschluessel\")['v'][0:5]] \n",
    "    except:\n",
    "        #to be defined as missing values\n",
    "        lk = lk + ['99999'] \n",
    "\n",
    "#adding district code to data frame and save\n",
    "w_stations['GemKey5'] = lk\n",
    "w_stations.head()\n",
    "#w_stations.to_csv('data sources/weather_stations_GemKey.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_stations.to_csv('data sources/weather_stations_GemKey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retreive weather data for each district\n",
    "\n",
    "Now, we collect the relevant weather data from all listed stations on the district level. If there are more than one weather stations in a certain district, mean values were computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to DWD open data portal\n",
    "path = 'https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/monthly/kl/historical/'\n",
    "\n",
    "# read all files in directory and put them in a list\n",
    "# weather data is provided with zip files\n",
    "site_content = requests.get(path).text\n",
    "soup = BeautifulSoup(site_content, 'html.parser').find_all('a')\n",
    "files = [x.attrs['href'] for x in soup]\n",
    "\n",
    "# Liste bereinigen: nur zip-Dateien\n",
    "zipfiles = []\n",
    "for i in files:\n",
    "    if i[-3:] == 'zip':\n",
    "        zipfiles.append(i)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# empty data frame for merging masked data\n",
    "empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function opens the zip file of a given weather station (Station ID),\n",
    "# extracts all relevant weather indicators, and returns them as dictionary\n",
    "\n",
    "def get_indicators(statID):\n",
    "    statID = '_' + statID + '_'\n",
    "\n",
    "    for i in zipfiles:\n",
    "        if (statID in i) == True:\n",
    "            hit = i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # complete URL\n",
    "    filepath = path+hit\n",
    "\n",
    "    goal = urlopen(filepath)\n",
    "    zippedfiles = zipfile.ZipFile(BytesIO(goal.read()))\n",
    "\n",
    "    for i in zippedfiles.namelist():\n",
    "        if ('produkt_klima_monat' in i) == True:\n",
    "            datafile = i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # unpack files and write in data frame\n",
    "    data = zippedfiles.read(datafile).decode()\n",
    "    df = pd.read_csv(StringIO(data), sep=';')\n",
    "\n",
    "    # MO_TT: Monatsmittel der tägl. Lufttemperatur in 2m Höhe\n",
    "    # MO_SD_S: Monatssumme der Sonnenscheindauer\n",
    "    # MO_FK: Monatsmittel der tägl. Windstärke\n",
    "    # MO_RR: Monatssumme der Niederschlagshöhe\n",
    "\n",
    "    df = df[df['MESS_DATUM_BEGINN']>19990000][['MESS_DATUM_BEGINN', 'MO_TT', 'MO_SD_S', 'MO_FK', 'MO_RR']]\n",
    "    df = df[df['MESS_DATUM_BEGINN']<20210000]\n",
    "\n",
    "    # get year and month\n",
    "    df['Jahr'] = [str(x)[0:4] for x in df['MESS_DATUM_BEGINN']]\n",
    "    df['Monat'] = [str(x)[4:6] for x in df['MESS_DATUM_BEGINN']]\n",
    "    df['Jahr'] = pd.to_numeric(df['Jahr'])\n",
    "    df['Monat'] = pd.to_numeric(df['Monat'])\n",
    "\n",
    "    df.drop(['MESS_DATUM_BEGINN'], axis=1, inplace=True)\n",
    "\n",
    "    # missing values\n",
    "    df.replace({-999: np.nan}, inplace=True)\n",
    "\n",
    "    # bringing stuff together\n",
    "    empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "    \n",
    "    df_TT = df.pivot(index='Jahr', columns='Monat', values='MO_TT')\n",
    "    df_SD = df.pivot(index='Jahr', columns='Monat', values='MO_SD_S')\n",
    "    df_FK = df.pivot(index='Jahr', columns='Monat', values='MO_FK')\n",
    "    df_RR = df.pivot(index='Jahr', columns='Monat', values='MO_RR')\n",
    "    df_TT = empty_df.fillna(df_TT)\n",
    "    df_SD = empty_df.fillna(df_SD)\n",
    "    df_FK = empty_df.fillna(df_FK)\n",
    "    df_RR = empty_df.fillna(df_RR)\n",
    "\n",
    "    # as Dictionary\n",
    "    station_indicators = {'TT': df_TT, 'SD': df_SD, 'FK': df_FK, 'RR': df_RR}\n",
    "    \n",
    "    return station_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there is more than one weather station in a particular district,\n",
    "# this functions comines the values\n",
    "\n",
    "def combine_stations(stat_ids):\n",
    "    \n",
    "    station_ids = stat_ids\n",
    "    station_values = {}\n",
    "\n",
    "    #if more than one weather station in district\n",
    "    if len(station_ids) > 1:\n",
    "\n",
    "        for i in station_ids:\n",
    "            station_values[i] = get_indicators(i)\n",
    "\n",
    "        TT_list = []\n",
    "        SD_list = []\n",
    "        FK_list = []\n",
    "        RR_list = []\n",
    "\n",
    "        for i in station_values.keys():\n",
    "            TT_list = TT_list + [station_values[i]['TT']]\n",
    "            SD_list = SD_list + [station_values[i]['SD']]\n",
    "            FK_list = FK_list + [station_values[i]['FK']]\n",
    "            RR_list = RR_list + [station_values[i]['RR']]\n",
    "\n",
    "        raw_df_TT = pd.concat(TT_list)\n",
    "        raw_df_SD = pd.concat(SD_list)\n",
    "        raw_df_FK = pd.concat(FK_list)\n",
    "        raw_df_RR = pd.concat(RR_list)\n",
    "\n",
    "        empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "        df_TT_full = empty_df.copy()\n",
    "        df_SD_full = empty_df.copy()\n",
    "        df_FK_full = empty_df.copy()\n",
    "        df_RR_full = empty_df.copy()\n",
    "\n",
    "        for i in empty_df.index:\n",
    "            df_TT_full.loc[i] = raw_df_TT.loc[i].mean(axis=0)\n",
    "            df_SD_full.loc[i] = raw_df_SD.loc[i].mean(axis=0)\n",
    "            df_FK_full.loc[i] = raw_df_FK.loc[i].mean(axis=0)\n",
    "            df_RR_full.loc[i] = raw_df_RR.loc[i].mean(axis=0)\n",
    "    \n",
    "    #if just one weather station in district\n",
    "    else:\n",
    "        _ = get_indicators(station_ids[0])\n",
    "        df_TT_full = _['TT']\n",
    "        df_SD_full = _['SD']\n",
    "        df_FK_full = _['FK']\n",
    "        df_RR_full = _['RR']\n",
    "\n",
    "    return {'TT':df_TT_full, 'SD':df_SD_full, 'FK':df_FK_full, 'RR':df_RR_full}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with district keys and weather stations id\n",
    "gemkeys = w_stations['GemKey5'].unique().tolist()\n",
    "gemkey_stat = {x: w_stations['Stations_id'][w_stations['GemKey5']==x].values.tolist() for x in gemkeys}\n",
    "\n",
    "# create dictionary of dictionaries with weather data for each district as the mean\n",
    "# of all data from the weather stations of this district\n",
    "weather_district = {}\n",
    "for i in gemkey_stat.keys():\n",
    "    try:\n",
    "        weather_district[i] = combine_stations(gemkey_stat[i])\n",
    "    except:\n",
    "        weather_district[i] = {'TT':empty_df, 'SD':empty_df, 'FK':empty_df, 'RR':empty_df}\n",
    "\n",
    "# save to file\n",
    "with open('data sources/weather_district.pkl', 'wb') as file:\n",
    "    pickle.dump(weather_district, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retreive soil data for each district\n",
    "\n",
    "Soil data (soil moisture index, SMI) is collected from the \"UFZ Dürremonitor\". UFZ provides the SMI for the surface (20cm) and deeper ground (180cm). The datasets can be downloded here: https://www.ufz.de/index.php?de=37937\n",
    "\n",
    "However, the data is stored in netcdf files and the geographical information is provided by coordinates using the Gauß-Krüger-Zone. So we have to transform it to longitude and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from pyproj import Transformer\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# empty data frame to fill later\n",
    "empty_df = pd.DataFrame(index=(np.arange(1999, 2021)), columns=(np.arange(1, 13)))\n",
    "\n",
    "# reading UFZ data set\n",
    "ds_ob = nc.Dataset('data sources/248980_SMI_SM_L02_Oberboden_monatlich_1951-2020_inv.nc')\n",
    "ds_gb = nc.Dataset('data sources/248981_SMI_SM_Lall_Gesamtboden_monatlich_1951-2020_inv.nc')\n",
    "\n",
    "# UFZ data is grid data\n",
    "# transform from lat/lon (EPSG:4326) to Gauß-Krüger Zone 4 (EPSG: 31468)\n",
    "transformer = Transformer.from_crs(4326, 31468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smi (ds, lat, lon):\n",
    "    # transform\n",
    "    nor, eas = transformer.transform(lat, lon)\n",
    "    \n",
    "    # find nearest point\n",
    "    near_north = min(ds['northing'][:], key=lambda x:abs(x-nor))\n",
    "    near_east = min(ds['easting'][:], key=lambda x:abs(x-eas))\n",
    "\n",
    "    # get index\n",
    "    ix_northing = list(ds['northing']).index(near_north)\n",
    "    ix_easting = list(ds['easting']).index(near_east)\n",
    "\n",
    "    #read SMIi for region\n",
    "    smi_raw = ds['SMI'][:, ix_northing, ix_easting].data\n",
    "\n",
    "    # sort list by years\n",
    "    smi_years = np.reshape(smi_raw, [70, 12])\n",
    "\n",
    "    # set year as index for data frame\n",
    "    smi = pd.DataFrame(smi_years, columns=[np.arange(1, 13)], index=np.arange(1951, 2021))\n",
    "\n",
    "    return smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_smi(ds, GemKey5):\n",
    "    ob_full = empty_df.copy()\n",
    "\n",
    "    smi_values =[]\n",
    "\n",
    "    if len(gemkey_stat[GemKey5]) > 1:\n",
    "        \n",
    "        dstrct_stations = np.array(w_stations[['geoBreite', 'geoLaenge']][w_stations['GemKey5']==GemKey5])\n",
    "           \n",
    "        for i in dstrct_stations:\n",
    "            lat = i[0]\n",
    "            lon = i[1]\n",
    "\n",
    "            try:\n",
    "                smi_values = smi_values + [get_smi(ds, lat, lon).loc[1999:2020].replace(-9999, np.nan)]\n",
    "            except:\n",
    "                smi_values = smi_values + [empty_df.copy()]\n",
    "\n",
    "        try:\n",
    "            ob_raw = pd.concat(smi_values)\n",
    "            for i in empty_df.index:\n",
    "                ob_full.loc[i] = ob_raw.loc[i].mean(axis=0).tolist()\n",
    "        except:\n",
    "            ob_full = empty_df.copy()\n",
    "    \n",
    "    else:\n",
    "        dstrct_stations = np.array(w_stations[['geoBreite', 'geoLaenge']][w_stations['GemKey5']==GemKey5])\n",
    "        lat = dstrct_stations[0][0]\n",
    "        lon = dstrct_stations[0][1]\n",
    "        try:\n",
    "            ob_full = get_smi(ds, lat, lon).loc[1999:2020].replace(-9999, np.nan)\n",
    "        except:\n",
    "            ob_full = empty_df.copy()\n",
    "    \n",
    "    return ob_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with district keys and weather stations id\n",
    "gemkeys = w_stations['GemKey5'].unique().tolist()\n",
    "gemkey_stat = {x: w_stations['Stations_id'][w_stations['GemKey5']==x].values.tolist() for x in gemkeys}\n",
    "\n",
    "# collect SMI values for each German district\n",
    "smi_ob = {}\n",
    "smi_gb = {}\n",
    "for i in gemkey_stat.keys():\n",
    "    smi_ob[i] = combine_smi(ds_ob, i)\n",
    "    smi_gb[i] = combine_smi(ds_gb, i)\n",
    "\n",
    "# append smi data frames to weather data frames\n",
    "for i in weather_district.keys():\n",
    "    weather_district[i]['SMI_OB'] = smi_ob[i].copy()\n",
    "    weather_district[i]['SMI_GB'] = smi_gb[i].copy()\n",
    "\n",
    "# saving final data set\n",
    "with open('data sources/weather_district.pkl', 'wb') as file:\n",
    "    pickle.dump(weather_district, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create filter\n",
    "\n",
    "* filter is used for crop-specific growth periods later\n",
    "    * <tt>df</tt>: data frame with weather data\n",
    "    * <tt>months</tt>: list of months of growing season i.e. <tt>[10, 11, 12, 1, 2, 3]</tt>\n",
    "    * <tt>cropyear</tt>: year of the harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_growingseason(df, months, cropyear):\n",
    "    start = months[0]-1\n",
    "    stop  = months[-1]\n",
    "\n",
    "    if months[0] > months[-1]:\n",
    "        #empty filter\n",
    "        filtr_clean = np.repeat(np.nan, 24).reshape(2, 12)\n",
    "\n",
    "        #active filter\n",
    "        filtr = filtr_clean\n",
    "        filtr[0][start:] = 1\n",
    "        filtr[1][:stop]  = 1\n",
    "        \n",
    "        dx = df.loc[(cropyear-1): cropyear].copy()\n",
    "        dx = dx * filtr\n",
    "        dx = dx.values.flatten().tolist()\n",
    "        dx = np.array([x for x in dx if x > 0])      \n",
    "\n",
    "    else:\n",
    "        #empty filter\n",
    "        filtr_clean = np.repeat(np.nan, 12)\n",
    "\n",
    "        #active filter\n",
    "        filtr = filtr_clean\n",
    "        filtr[start:stop] = 1\n",
    "        \n",
    "        dx = df.loc[cropyear].copy()\n",
    "        dx = dx * filtr\n",
    "        dx = np.array(dx.dropna().tolist())\n",
    "    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 47.2 , 101.  ,  55.  ,  99.85, 152.5 ])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = weather_district['09275']['RR']\n",
    "croptime = np.array([11, 12, 1, 2, 3])\n",
    "apply_growingseason(x, croptime, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
